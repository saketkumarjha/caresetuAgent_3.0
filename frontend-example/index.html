<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CareSetu Voice Agent - Frontend Example</title>
    <script src="https://unpkg.com/livekit-client"></script>
    <style>
      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        flex-direction: column;
        align-items: center;
        min-height: 100vh;
        background-color: #f5f7fa;
      }
      .container {
        width: 100%;
        max-width: 800px;
        padding: 20px;
        box-sizing: border-box;
      }
      header {
        background-color: #4a6bff;
        color: white;
        width: 100%;
        padding: 20px;
        text-align: center;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
      }
      .card {
        background-color: white;
        border-radius: 10px;
        padding: 20px;
        margin: 20px 0;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }
      button {
        background-color: #4a6bff;
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 5px;
        cursor: pointer;
        font-size: 16px;
        margin: 10px 0;
        transition: background-color 0.3s;
      }
      button:hover {
        background-color: #3a5bef;
      }
      button:disabled {
        background-color: #cccccc;
        cursor: not-allowed;
      }
      .status {
        margin: 10px 0;
        padding: 10px;
        border-radius: 5px;
        background-color: #f0f0f0;
      }
      .controls {
        display: flex;
        flex-direction: column;
        gap: 10px;
      }
      .audio-visualizer {
        width: 100%;
        height: 50px;
        background-color: #f0f0f0;
        border-radius: 5px;
        margin: 10px 0;
        position: relative;
        overflow: hidden;
      }
      .audio-bar {
        position: absolute;
        bottom: 0;
        width: 3px;
        background-color: #4a6bff;
        margin-right: 2px;
      }
      .transcript {
        max-height: 200px;
        overflow-y: auto;
        padding: 10px;
        background-color: #f9f9f9;
        border-radius: 5px;
        margin: 10px 0;
      }
      .user-speech {
        color: #4a6bff;
        margin-bottom: 5px;
      }
      .agent-speech {
        color: #333;
        margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>CareSetu Voice Agent</h1>
      <p>Healthcare Voice Assistant with Appointment Booking</p>
    </header>

    <div class="container">
      <div class="card">
        <h2>Voice Assistant</h2>
        <div class="status" id="connectionStatus">Status: Disconnected</div>

        <div class="audio-visualizer" id="audioVisualizer">
          <!-- Audio bars will be added dynamically -->
        </div>

        <div class="transcript" id="transcript">
          <!-- Transcript will appear here -->
        </div>

        <div class="controls">
          <button id="connectButton">Connect to Agent</button>
          <button id="startButton" disabled>Start Conversation</button>
          <button id="stopButton" disabled>End Conversation</button>
        </div>
      </div>

      <div class="card">
        <h2>Quick Actions</h2>
        <button class="action-button" data-action="book">
          Book an Appointment
        </button>
        <button class="action-button" data-action="check">
          Check Availability
        </button>
        <button class="action-button" data-action="cancel">
          Cancel Appointment
        </button>
      </div>
    </div>

    <script>
      // LiveKit configuration
      const LIVEKIT_URL = "wss://assemblyaiproject-2oo3ntng.livekit.cloud"; // Your LiveKit URL
      const TOKEN = "YOUR_TOKEN"; // You'll need to generate this on your server

      // DOM elements
      const connectButton = document.getElementById("connectButton");
      const startButton = document.getElementById("startButton");
      const stopButton = document.getElementById("stopButton");
      const connectionStatus = document.getElementById("connectionStatus");
      const transcript = document.getElementById("transcript");
      const audioVisualizer = document.getElementById("audioVisualizer");
      const actionButtons = document.querySelectorAll(".action-button");

      // LiveKit variables
      let room;
      let localAudioTrack;
      let remoteAudioTrack;
      let isConnected = false;
      let isConversationActive = false;

      // Create audio bars for visualizer
      for (let i = 0; i < 50; i++) {
        const bar = document.createElement("div");
        bar.className = "audio-bar";
        bar.style.left = `${i * 5}px`;
        bar.style.height = "0px";
        audioVisualizer.appendChild(bar);
      }

      // Connect to LiveKit
      connectButton.addEventListener("click", async () => {
        try {
          connectionStatus.textContent = "Status: Connecting...";

          // Create a new room
          room = new LivekitClient.Room();

          // Set up event listeners
          room.on(
            LivekitClient.RoomEvent.TrackSubscribed,
            (track, publication, participant) => {
              if (track.kind === "audio") {
                remoteAudioTrack = track;
                const audioElement = track.attach();
                audioElement.style.display = "none";
                document.body.appendChild(audioElement);

                // Add agent's first message
                addToTranscript(
                  "Hello! I'm your CareSetu healthcare assistant. How can I help you today?",
                  "agent"
                );
              }
            }
          );

          room.on(LivekitClient.RoomEvent.Disconnected, () => {
            connectionStatus.textContent = "Status: Disconnected";
            isConnected = false;
            updateButtonStates();
          });

          // Connect to the room
          await room.connect(LIVEKIT_URL, TOKEN);

          // Enable local audio
          localAudioTrack = await LivekitClient.createLocalAudioTrack();
          await room.localParticipant.publishTrack(localAudioTrack);

          // Update UI
          connectionStatus.textContent = "Status: Connected";
          isConnected = true;
          updateButtonStates();

          // Set up audio visualization
          setupAudioVisualization(localAudioTrack);
        } catch (error) {
          console.error("Connection error:", error);
          connectionStatus.textContent = `Status: Connection failed - ${error.message}`;
        }
      });

      // Start conversation
      startButton.addEventListener("click", () => {
        if (!isConnected) return;

        isConversationActive = true;
        updateButtonStates();
        connectionStatus.textContent = "Status: Conversation active";

        // In a real implementation, you would signal to your agent to start listening
      });

      // Stop conversation
      stopButton.addEventListener("click", async () => {
        if (!isConnected) return;

        isConversationActive = false;
        updateButtonStates();
        connectionStatus.textContent = "Status: Connected (conversation ended)";

        // In a real implementation, you would signal to your agent to stop listening
      });

      // Quick action buttons
      actionButtons.forEach((button) => {
        button.addEventListener("click", () => {
          if (!isConnected || !isConversationActive) return;

          const action = button.dataset.action;
          let message = "";

          switch (action) {
            case "book":
              message = "I'd like to book an appointment";
              break;
            case "check":
              message = "What appointment times are available tomorrow?";
              break;
            case "cancel":
              message = "I need to cancel my appointment";
              break;
          }

          // Add user message to transcript
          addToTranscript(message, "user");

          // In a real implementation, you would send this message to your agent
          // For demo purposes, we'll simulate a response after a delay
          setTimeout(() => {
            let response = "";
            switch (action) {
              case "book":
                response =
                  "I'd be happy to help you book an appointment! What day and time works best for you?";
                break;
              case "check":
                response =
                  "Let me check what times are available tomorrow. I have slots at 10:00 AM, 1:30 PM, and 3:45 PM. Would any of these work for you?";
                break;
              case "cancel":
                response =
                  "I can help you cancel your appointment. Could you please provide your name and the date of your appointment?";
                break;
            }
            addToTranscript(response, "agent");
          }, 1000);
        });
      });

      // Helper functions
      function updateButtonStates() {
        connectButton.disabled = isConnected;
        startButton.disabled = !isConnected || isConversationActive;
        stopButton.disabled = !isConnected || !isConversationActive;

        actionButtons.forEach((button) => {
          button.disabled = !isConnected || !isConversationActive;
        });
      }

      function addToTranscript(text, speaker) {
        const messageElement = document.createElement("div");
        messageElement.className =
          speaker === "user" ? "user-speech" : "agent-speech";
        messageElement.textContent =
          speaker === "user" ? `You: ${text}` : `Agent: ${text}`;
        transcript.appendChild(messageElement);
        transcript.scrollTop = transcript.scrollHeight;
      }

      function setupAudioVisualization(audioTrack) {
        if (!audioTrack) return;

        const audioContext = new (window.AudioContext ||
          window.webkitAudioContext)();
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;

        const source = audioContext.createMediaStreamSource(
          new MediaStream([audioTrack.mediaStreamTrack])
        );
        source.connect(analyser);

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const bars = audioVisualizer.querySelectorAll(".audio-bar");

        function animate() {
          if (!isConnected) return;

          requestAnimationFrame(animate);
          analyser.getByteFrequencyData(dataArray);

          for (let i = 0; i < bars.length; i++) {
            const index = Math.floor((i * bufferLength) / bars.length);
            const value = dataArray[index] / 2;
            bars[i].style.height = `${value}px`;
          }
        }

        animate();
      }
    </script>
  </body>
</html>
